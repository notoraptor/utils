Highlights
- Moved Python 3.* minimum supported version from 3.3 to 3.4
- Replaced deprecated package ``nose-parameterized`` with up-to-date package ``parameterized`` for Theano requirements
- Support more debuggers for PdbBreakpoint
- Faster compilation step by massively using a new interface for op params
- Faster optimization step
- Documentation updated and more complete
- Many bug fixes, and crash fixes and warning improvements

Development
- Added a params type `ParamsType` to ease usage of op params and help wrap many op params into one python/c struct
- Added new op param type EnumType, EnumList and CEnumType to help manage enumerations in Python and C code.
- Now use same standard macro names in both CPU and new backend (DTYPE_INPUT_*, DTYPE_OUTPUT_*, etc.)
- [internal] Insertion of an OutputGuard is now considered as an error.
- We start to move C code files into separate folder ``c_code`` in every Theano module
- Many improvements for TRAVIS CI tests (with better splitting for faster testing)
- Many improvements for Jenkins CI tests: support for Mac and Windows testings, usage of Docker for better tests isolation

New features
- Added new prop `replace` for `choicefromuniform` op
- Added options for `disconnected_outputs` to rop
- Added a wrapper for baidu's CTC
- Added gradient for matrix pseudoinverse op
- Added modes half and full for Images2Neibs ops.
- Added Scaled Exponential Linear Unit (SELU) activation
- Added separable convolutions
- Implementeded grouped convolutions
- Added sigmoid_binary_crossentropy function
- Added the tri-gamma function
- Implemented grad for AbstractBatchNormTrainGrad
- Added ``on_error`` option for cholesky function
- Added new theano flag cmodule.debug to create a debug mode for theano C code.
- Added new theano flag deterministic. to help control how Theano optimize certain ops that have deterministic versions. Currently used for subtensor Ops only.
- Added new theano flage cycle_detection that allows to speed-up compilation step by reducing time spending in inplace optimization step.
- Added new theano flag check_stack_trace to help check the stack trace during optimization process.

cuDNN
- Official support for versions >= v5.1 and >= v6.0
- Better support and loading on Windows and Mac
- Added support for cuDNN v6 dilated convolutions
- Added reduction based on cuDNN with GpuDnnReduction op (for v6 and later)

GPU
- Removed old backend ``theano.sandbox.cuda``
- Prevent GPU initialization when not required
- GPU SoftMax works with both OpenCL and CUDA
- Added disk caching option for new backend kernels
- Added Cholesky op based on ``cusolver`` backend
- Added GPU ops based on magma library: SVD, matrix inverse, QR, cholesky and eigh
- Added GpuAdvancedIncSubtensor
- Added GpuCublasTriangularSolve
- Added atomic addition and exchange for long long values in ``GpuAdvancedIncSubtensor1_dev20``
- Support of k offset parameter for ``GpuEye``
- CrossentropyCategorical1Hot and its gradient are now lifted to GPU
- ``float16`` now enabled for GpuGemmBatch
- Log gamma function now works on new backend
- We start to avoid lifting ops that don't support float16 on GPU

Interface changes
- Replaced MultinomialWOReplacementFromUniform -> ChoiceFromUniform
- Removed COp.get_op_params()
- Support of list of strings for Op.c_support_code(), to help not duplicate support codes

Other changes
- Added deprecation warning for the softmax and logsoftmax vector case
- Removed ``theano/compat/six.py``

